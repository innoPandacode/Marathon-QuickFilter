# scraper.py
# -*- coding: utf-8 -*-
import os
import io
import re
import urllib.parse
from datetime import datetime

import pandas as pd
import requests
import streamlit as st
from bs4 import BeautifulSoup
from reportlab.lib import colors
from reportlab.lib.pagesizes import A4, landscape
from reportlab.lib.styles import ParagraphStyle
from reportlab.platypus import SimpleDocTemplate, Table, TableStyle, Paragraph
from reportlab.pdfbase import pdfmetrics
from reportlab.pdfbase.ttfonts import TTFont

st.set_page_config(page_title="è‡ºåŒ—é¦¬æ‹‰æ¾è³½äº‹çˆ¬èŸ²", layout="wide")


# 1. å–å¾—æª”æ¡ˆæ‰€åœ¨ç›®éŒ„ï¼Œå†æ‹¼å‡º MSJH.TTC çš„è·¯å¾‘
FONT_PATH = os.path.join(os.path.dirname(__file__), "MSJH.TTC")

# 2. å˜—è©¦è¨»å†Šå­—å‹â”€â”€è‹¥æ‰¾ä¸åˆ°æª”æ¡ˆæˆ–ç™¼ç”Ÿå…¶ä»–éŒ¯èª¤ï¼Œå°±ç”¨ warning æç¤º
try:
    pdfmetrics.registerFont(TTFont("MSJH-Light", FONT_PATH))
    pdfmetrics.registerFont(TTFont("MSJH-Bold", FONT_PATH))
    pdfmetrics.registerFontFamily(
        "MSJH",
        normal="MSJH-Light", bold="MSJH-Bold",
        italic="MSJH-Light", boldItalic="MSJH-Bold"
    )
except Exception as e:
    st.warning(
        f"âš ï¸ è¨»å†Šã€Œå¾®è»Ÿæ­£é»‘é«”ã€å­—å‹æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}\n"
        "è«‹ç¢ºèª msJh.ttc æ˜¯å¦å­˜åœ¨æ–¼ç³»çµ±å¯è®€è·¯å¾‘ã€‚"
    )

# å»ºç«‹ ParagraphStyleï¼Œä¾› PDF å…§æ‰€æœ‰ä¸­æ–‡å­—æ ¼ä½¿ç”¨
chinese_style = ParagraphStyle(
    name="ChineseStyle",
    fontName="MSJH-Regular",
    fontSize=10,    # å¾ŒçºŒä¾æ¬„ä½æ•¸é‡å‹•æ…‹èª¿æ•´
    leading=12,
    wordWrap="CJK",
    alignment=1
)

# ===========================
# 1. æ¨¡æ“¬ ASP.NET PostBack çš„å‡½å¼
# ===========================
def get_filtered_soup(year: str, region: str, rtype: str):
    base_url = "http://www.taipeimarathon.org.tw/contest.aspx"
    session = requests.Session()

    # 1) åˆæ¬¡ GET
    try:
        r0 = session.get(base_url, timeout=10)
    except Exception as e:
        raise Exception(f"ç„¡æ³•é€£åˆ° {base_url}ï¼š{e}")
    if r0.status_code != 200:
        raise Exception(f"ç¬¬ä¸€æ¬¡ GET æ™‚ï¼ŒHTTP ç‹€æ…‹ç¢¼ {r0.status_code}ï¼Œé 200ã€‚")
    soup = BeautifulSoup(r0.text, "html.parser")

    # 2) æŠ“ä¸‰å€‹ <select> çš„ name å±¬æ€§
    sel_year = soup.find("select", id="Year")
    sel_region = soup.find("select", id="DropDownList1")
    sel_type = soup.find("select", id="type")

    if sel_year is None:
        raise Exception("æ‰¾ä¸åˆ° <select id='Year'>ï¼Œå¯èƒ½ç¶²ç«™æ¶æ§‹æ”¹ç‰ˆã€‚")
    if sel_region is None:
        raise Exception("æ‰¾ä¸åˆ° <select id='DropDownList1'> (è¡Œæ”¿å€)ã€‚")
    if sel_type is None:
        raise Exception("æ‰¾ä¸åˆ° <select id='type'> (è³½äº‹é¡å‹)ã€‚")

    year_name = sel_year.get("name")
    region_name = sel_region.get("name")
    type_name = sel_type.get("name")
    if not year_name or not region_name or not type_name:
        raise Exception("ç„¡æ³•æ“·å–ä¸‰å€‹ <select> çš„ name å±¬æ€§ï¼Œå¯èƒ½ HTML æ”¹ç‰ˆã€‚")

    # 3) æ‹¿ <option selected> çš„é è¨­å€¼
    default_year_opt = sel_year.find("option", selected=True)
    default_region_opt = sel_region.find("option", selected=True)
    default_type_opt = sel_type.find("option", selected=True)
    if default_year_opt is None or default_region_opt is None or default_type_opt is None:
        raise Exception("æŸäº› <select> æ‰¾ä¸åˆ°è¢«é¸ä¸­çš„ <option>ã€‚")
    default_year_val = default_year_opt.get("value", "")
    default_region_val = default_region_opt.get("value", "")
    default_type_val = default_type_opt.get("value", "")

    # Helper: extract hidden inputs
    def extract_hidden_inputs(soup_inner):
        form = soup_inner.find("form", id="aspnetForm")
        if form is None:
            form = soup_inner.find("form")
            if form is None:
                raise Exception("é é¢ä¸Šæ‰¾ä¸åˆ°ä»»ä½• <form>ï¼Œç„¡æ³•æ“·å–éš±è—æ¬„ä½ (VIEWSTATE)ã€‚")
        data = {}
        for inp in form.find_all("input", {"type": "hidden"}):
            if inp.has_attr("name"):
                data[inp["name"]] = inp.get("value", "")
        return data

    # Helper: PostBack
    def postback(curr_soup, event_target_name: str, selected_values: dict):
        payload = extract_hidden_inputs(curr_soup)
        for k, v in selected_values.items():
            payload[k] = v
        payload["__EVENTTARGET"] = event_target_name
        payload["__EVENTARGUMENT"] = ""
        try:
            r = session.post(base_url, data=payload, timeout=10)
        except Exception as e:
            raise Exception(f"PostBack æ™‚ç¶²è·¯éŒ¯èª¤ï¼š{e}")
        if r.status_code != 200:
            raise Exception(f"PostBack (EVENTTARGET={event_target_name}) æ™‚ï¼ŒHTTP {r.status_code}ã€‚")
        return BeautifulSoup(r.text, "html.parser")

    current_soup = soup

    # 4) è™•ç†å¹´ä»½
    if year not in ["now", "all"]:
        values = {
            year_name: year,
            region_name: default_region_val,
            type_name: default_type_val
        }
        current_soup = postback(current_soup, year_name, values)

    # 5) è™•ç†è¡Œæ”¿å€
    selected_region_val = region if region != "all" else default_region_val
    if selected_region_val not in ["all", default_region_val]:
        values = {
            year_name: year if year not in ["now", "all"] else default_year_val,
            region_name: selected_region_val,
            type_name: default_type_val
        }
        current_soup = postback(current_soup, region_name, values)

    # 6) è™•ç†è³½äº‹é¡å‹
    selected_type_val = rtype if rtype != "all" else default_type_val
    if selected_type_val not in ["all", default_type_val]:
        values = {
            year_name: year if year not in ["now", "all"] else default_year_val,
            region_name: selected_region_val,
            type_name: selected_type_val
        }
        current_soup = postback(current_soup, type_name, values)

    return current_soup


# ===========================
# 2. è§£æ HTML table â†’ DataFrame (å«è³½äº‹åç¨±è¶…é€£çµ)
# ===========================
def parse_table_to_df(soup):
    table = soup.find("table", id="ctl00_ContentPlaceHolder1_GridView1")
    if table is None:
        for tbl in soup.find_all("table"):
            header_cells = [th.get_text(strip=True) for th in tbl.find_all("th")]
            if "è³½äº‹åç¨±" in header_cells and "æ—¥æœŸ" in header_cells:
                table = tbl
                break
    if table is None:
        return pd.DataFrame()

    # å– header row
    header_row = table.find("tr")
    raw_headers = [cell.get_text(strip=True) for cell in header_row.find_all(["th", "td"])]
    valid_indices = [i for i, h in enumerate(raw_headers) if h.strip() != ""]
    headers = [raw_headers[i] for i in valid_indices]

    # æ‰¾ã€Œè³½äº‹åç¨±ã€ä½ç½®
    if "è³½äº‹åç¨±" not in headers:
        return pd.DataFrame()
    event_idx = headers.index("è³½äº‹åç¨±")

    data = []
    event_links = []

    for row in table.find_all("tr")[1:]:
        cells = row.find_all("td")
        if not cells:
            continue
        texts = [cell.get_text(separator=" ", strip=True) for cell in cells]
        if all(txt == "" for txt in texts):
            continue

        # 1) æ–‡å­—éƒ¨åˆ†
        filtered_texts = [texts[i] for i in valid_indices]
        data.append(filtered_texts)

        # 2) è³½äº‹åç¨±è¶…é€£çµ
        td_event = row.find_all("td")[valid_indices[event_idx]]
        a_tag = td_event.find("a", href=True)
        if a_tag:
            href = a_tag["href"]
            if href.startswith("http"):
                full_url = href
            else:
                full_url = urllib.parse.urljoin("http://www.taipeimarathon.org.tw/contest.aspx", href)
        else:
            full_url = ""
        event_links.append(full_url)

    df = pd.DataFrame(data, columns=headers)
    df["è³½äº‹é€£çµ"] = event_links

    # è§£æã€Œæ—¥æœŸã€
    def parse_event_date(event_name: str, date_str: str):
        year_val = datetime.now().year
        m_year = re.match(r"^\s*(\d{4})", event_name)
        if m_year:
            year_val = int(m_year.group(1))
        m2 = re.match(r"(\d{1,2})/(\d{1,2})", date_str)
        if not m2:
            return None
        month = int(m2.group(1))
        day = int(m2.group(2))
        try:
            return datetime(year=year_val, month=month, day=day)
        except ValueError:
            return None

    parsed_dates = []
    for _, r in df.iterrows():
        evn = r.get("è³½äº‹åç¨±", "")
        dt_txt = r.get("æ—¥æœŸ", "")
        parsed_dates.append(parse_event_date(evn, dt_txt))
    df["parsed_date"] = parsed_dates

    # ç”¢ç”Ÿåœ°åœ–é€£çµ
    def make_maps_link(addr: str) -> str:
        base = "https://www.google.com/maps/search/?api=1&query="
        return base + urllib.parse.quote(addr)

    def make_maps_embed(addr: str) -> str:
        base = "https://maps.google.com/maps?q="
        return base + urllib.parse.quote(addr) + "&output=embed"

    if "åœ°é»" in df.columns:
        df["åœ°é»é€£çµ"] = df["åœ°é»"].apply(make_maps_link)
        df["åœ°é»åµŒå…¥URL"] = df["åœ°é»"].apply(make_maps_embed)
    else:
        df["åœ°é»é€£çµ"] = ""
        df["åœ°é»åµŒå…¥URL"] = ""

    return df


# ===========================
# 3. Streamlit ä»‹é¢
# ===========================
st.title("ğŸƒâ€â™‚ï¸ è‡ºåŒ—é¦¬æ‹‰æ¾è³½äº‹æŸ¥è©¢")

st.markdown(
    """
- **èªªæ˜**ï¼šæ­¤ App æœƒè‡ªå‹•å‘ [è‡ºåŒ—é¦¬æ‹‰æ¾è³½äº‹åˆ—è¡¨](http://www.taipeimarathon.org.tw/contest.aspx) é€²è¡Œçˆ¬èŸ²ï¼Œ  
  åŒæ™‚æŠ“å–ã€Œè³½äº‹åç¨±ã€åŸå§‹é€£çµï¼Œä¸¦èƒ½åœ¨å‰ç«¯å’Œ PDF ç”Ÿæˆå¯é»æ“Šé€£çµã€‚  
- çµæœé¡¯ç¤ºåœ¨ä¸»ç•«é¢ï¼Œä¸¦å¯ç«‹å³é¸æ“‡åœ°é»é è¦½åœ°åœ–ï¼Œæˆ–ä¸‹è¼‰ PDFã€‚  
"""
)

with st.sidebar:
    st.header("ğŸ” ç¯©é¸æ¢ä»¶")

    year_options = ["all", "now"] + [str(y) for y in range(2005, 2026)]
    year_display = {
        "all": "æ­·å²è³½äº‹ (å…¨éƒ¨å¹´ä»½)",
        "now": "ç›®å‰è³½äº‹",
        **{str(y): str(y) for y in range(2005, 2026)}
    }
    year_sel = st.selectbox("å¹´ä»½", options=year_options, format_func=lambda x: year_display[x])

    region_options = ["all", "åŒ—", "ä¸­", "å—", "æ±", "å…¶ä»–"]
    region_display = {
        "all": "å…¨éƒ¨",
        "åŒ—": "åŒ—éƒ¨",
        "ä¸­": "ä¸­éƒ¨",
        "å—": "å—éƒ¨",
        "æ±": "æ±éƒ¨",
        "å…¶ä»–": "å…¶ä»–"
    }
    region_sel = st.selectbox("è¡Œæ”¿å€", options=region_options, format_func=lambda x: region_display[x])

    type_options = ["public", "all", "1", "2", "3", "4", "5", "6", "7", "8"]
    type_display = {
        "public": "å…¨åœ‹è³½æœƒ",
        "all": "å…¨éƒ¨",
        "1": "è¶…ç´šé¦¬æ‹‰æ¾",
        "2": "é¦¬æ‹‰æ¾",
        "3": "åŠç¨‹é¦¬æ‹‰æ¾",
        "4": "10k~åŠé¦¬",
        "5": "10kä»¥ä¸‹",
        "6": "ä¼‘é–’æ´»å‹•",
        "7": "éµäººè³½",
        "8": "æ¥åŠ›è³½"
    }
    type_sel = st.selectbox("è³½äº‹é¡å‹", options=type_options, format_func=lambda x: type_display[x])

    st.markdown("---")
    keyword = st.text_input("é—œéµå­— (å¯ç©ºç™½)", placeholder="ä¾‹å¦‚ã€Œè‡ºå—ã€")

    if st.button("é–‹å§‹æŸ¥è©¢"):
        with st.spinner("è³‡æ–™æ“·å–ä¸­ï¼Œè«‹ç¨å€™â€¦"):
            try:
                final_soup = get_filtered_soup(year_sel, region_sel, type_sel)
                if final_soup is None:
                    st.error("âŒ å–å¾—ç¯©é¸å¾Œçš„é é¢å›å‚³ Noneã€‚è«‹ç¨å¾Œå†è©¦ã€‚")
                else:
                    df = parse_table_to_df(final_soup)
                    if df.empty:
                        st.warning("âš ï¸ æŸ¥ç„¡ä»»ä½•è³½äº‹ã€‚è«‹æª¢æŸ¥ç¯©é¸æ¢ä»¶æˆ–æ”¹æˆã€Œç›®å‰è³½äº‹ã€ã€‚")
                    else:
                        if keyword.strip() != "":
                            mask = df.apply(lambda r: r.astype(str).str.contains(keyword).any(), axis=1)
                            df = df[mask]
                        st.session_state["df"] = df
            except Exception as e:
                st.error(f"âŒ å–å¾—æˆ–è§£ææ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")

# ä¸»ç•«é¢ï¼šé¡¯ç¤ºæŸ¥è©¢çµæœ
if "df" in st.session_state and not st.session_state["df"].empty:
    df_all = st.session_state["df"]

    # 1. å…ˆæ’åºï¼Œä¸è¦å‹•åŸå§‹ session_state["df"]
    df_sorted = df_all.sort_values(by=["parsed_date"], ascending=True, na_position="last")

    # 2. å»ºç«‹ã€Œåœ°é» â†’ åœ°é»åµŒå…¥URLã€çš„æ˜ å°„ï¼Œä¾›åœ°åœ–é è¦½ä½¿ç”¨
    location_to_embed = dict(zip(df_sorted["åœ°é»"], df_sorted["åœ°é»åµŒå…¥URL"]))

    # 3. æ¥è‘—å°‡ display_df è¨­ç‚ºæ’åºå¾Œçš„ DataFrameï¼Œä½†æŠŠ parsed_date èˆ‡ åœ°é»åµŒå…¥URL å…©æ¬„ä¸€èµ· drop
    display_df = df_sorted.drop(columns=["parsed_date", "åœ°é»åµŒå…¥URL"])

    st.subheader("ğŸ“‹ æŸ¥è©¢çµæœ")
    st.dataframe(display_df, use_container_width=True)

    st.markdown("---")
    # 4. åœ°é»é è¦½ï¼šå› ç‚ºæˆ‘å€‘å·²ç¶“å¾ df_sorted å»ºäº† location_to_embedï¼Œé€™è£¡æ”¹å¾æ˜ å°„å– url
    st.subheader("ğŸ—ºï¸ åœ°é»é è¦½")
    addr_list = display_df["åœ°é»"].unique().tolist()
    sel_addr = st.selectbox("é¸æ“‡åœ°é»", options=addr_list, key="addr_selector")
    
    # å¾æ˜ å°„è¡¨å–å‡ºå°æ‡‰çš„åœ°åœ–åµŒå…¥ URL
    embed_url = location_to_embed.get(sel_addr, "")
    if embed_url:
        st.components.v1.iframe(embed_url, width=700, height=450)
    else:
        st.warning("âŒ ç„¡æ³•å–å¾—è©²åœ°é»çš„åµŒå…¥ URLã€‚")

    st.markdown("---")
    st.subheader("ğŸ“¥ ä¸‹è¼‰ PDF")

    # 5. ä»¥ä¸‹è™•ç† PDF ç”Ÿæˆï¼šdata_for_pdf åªéœ€è€ƒæ…® display_dfï¼Œå·²ç¶“ä¸å«ã€Œåœ°é»åµŒå…¥URLã€
    headers = list(display_df.columns)
    data_for_pdf = []

    # 5.a è™•ç†è¡¨é ­ï¼šç´”æ–‡å­— + ç°åº•
    header_row = []
    for col in headers:
        header_para = Paragraph(
            col,  # ç´”æ–‡å­—
            ParagraphStyle(
                name="HeaderStyle",
                fontName="MSJH-Regular",       # ä½¿ç”¨å¾®è»Ÿæ­£é»‘é«”å®¶æ—
                fontSize=10,
                leading=12,
                alignment=1,           # ç½®ä¸­
                textColor=colors.black,
                backColor=colors.HexColor("#D3D3D3")
            )
        )
        header_row.append(header_para)
    data_for_pdf.append(header_row)

    # 5.b è™•ç†è¡¨èº«ï¼šé‡å°ã€Œè³½äº‹é€£çµã€èˆ‡ã€Œåœ°é»é€£çµã€åšè¶…é€£çµï¼Œå…¶å®ƒç´”æ–‡å­—
    for row in display_df.itertuples(index=False):
        row_cells = []
        for i, value in enumerate(row):
            col_name = headers[i]

            if col_name == "è³½äº‹é€£çµ":
                url = value  # é€™è£¡çš„ value å°±æ˜¯åŸå§‹å ±åé é¢ URL
                if url.strip():
                    cell_para = Paragraph(
                        f'<link href="{url}">é€£çµ</link>',
                        chinese_style
                    )
                else:
                    cell_para = Paragraph("", chinese_style)

            elif col_name == "åœ°é»é€£çµ":
                url = value  # é€™è£¡çš„ value å°±æ˜¯ Google Map çš„ URL
                if url.strip():
                    cell_para = Paragraph(
                        f'<link href="{url}">åœ°åœ–é€£çµ</link>',
                        chinese_style
                    )
                else:
                    cell_para = Paragraph("", chinese_style)

            else:
                # å…¶é¤˜æ¬„ä½éƒ½ç”¨ç´”æ–‡å­—é¡¯ç¤º
                cell_para = Paragraph(str(value), chinese_style)

            row_cells.append(cell_para)
        data_for_pdf.append(row_cells)

    # 6. å»ºç«‹ PDFï¼šA4 æ©«å‘
    pdf_buffer = io.BytesIO()
    doc = SimpleDocTemplate(
        pdf_buffer,
        pagesize=landscape(A4),
        leftMargin=20, rightMargin=20, topMargin=20, bottomMargin=20
    )

    # 7. å‹•æ…‹è¨ˆç®—å­—å‹èˆ‡æ¬„å¯¬
    num_cols = len(headers)
    page_width = landscape(A4)[0] - doc.leftMargin - doc.rightMargin
    col_width = page_width / num_cols

    if col_width < 50:
        target_font_size = 6
    elif col_width < 80:
        target_font_size = 8
    else:
        target_font_size = 10

    chinese_style.fontSize = target_font_size
    chinese_style.leading = target_font_size + 2

    # 8. å»ºç«‹ Table ä¸¦è¨­å®šæ¬„å¯¬
    table = Table(data_for_pdf, colWidths=[col_width] * num_cols)

    # 9. è¨­å®š TableStyleï¼šç¶²æ ¼ç·šã€ç½®ä¸­ã€è¡¨é ­ padding
    table_style = TableStyle([
        ("GRID", (0, 0), (-1, -1), 0.5, colors.black),
        ("ALIGN", (0, 0), (-1, -1), "CENTER"),
        ("VALIGN", (0, 0), (-1, -1), "MIDDLE"),
        ("BOTTOMPADDING", (0, 0), (-1, 0), 6),
        ("TOPPADDING", (0, 0), (-1, 0), 6),
    ])
    table.setStyle(table_style)

    # 10. Build PDF
    doc.build([table])

    # 11. å–å¾— PDF bytes
    pdf_buffer.seek(0)
    pdf_bytes = pdf_buffer.read()
    pdf_buffer.close()

    # 12. çµ„æª”åï¼šã€Œå¹´ä»½_è¡Œæ”¿å€_è³½äº‹é¡å‹[_é—œéµå­—].pdfã€
    fn_year = year_sel if year_sel not in ["now", "all"] else "allYears"
    fn_region = region_sel
    fn_type = type_sel
    fn_keyword = keyword.strip().replace(" ", "_") if keyword.strip() != "" else ""
    filename_parts = [fn_year, fn_region, fn_type]
    if fn_keyword:
        filename_parts.append(fn_keyword)
    pdf_filename = "_".join(filename_parts) + ".pdf"

    # 13. æä¾›ä¸‹è¼‰æŒ‰éˆ•
    st.download_button(
        label="â¬‡ï¸ ä¸‹è¼‰æŸ¥è©¢çµæœ (PDF)",
        data=pdf_bytes,
        file_name=pdf_filename,
        mime="application/pdf"
    )